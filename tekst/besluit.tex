\chapter{Besluit}
\label{hoofdstuk:besluit}

We sluiten deze thesis af door terug te blikken op de belangrijkste conclusies van hoofdstukken \ref{hoofdstuk:tucker}, \ref{hoofdstuk:hervorming} en \ref{hoofdstuk:resultaten}. Ten slotte overlopen we ook nog even een aantal aspecten die mogelijk verder onderzocht zouden kunnen worden om onze compressie-algoritmen te verbeteren.

\section{Conclusies}

\subsection{Tucker-gebaseerde compressie}

In hoofdstuk \ref{hoofdstuk:tucker} onderzochten we de beste aanpak voor de verschillende fases van een compressie-algoritme gebaseerd op de Tucker-decompositie. Ten eerste is er de ST-HOSVD \cite{ref:st_hosvd}. Hierbij kozen we ervoor om de modes niet zozeer allemaal te verwerken in de volgorde van groot naar klein, maar eerst prioriteit te geven aan de spectrale mode vanwege diens erg goede comprimeerbaarheid. Daarnaast onderzochten we ook verschillende technieken om de SVD per mode sneller te berekenen zonder een significante fout toe te voegen. Uiteindelijk besloten we om te werken met de eigenwaardenontbinding van de Gram-matrix, hoewel ook combinaties werden overwogen met andere technieken zoals de \textit{randomized} SVD \cite{ref:randomized_svd} of een Lanczos-gebaseerde methode \cite{ref:lanczos} \cite{ref:saad}.\\

Daarna volgt orthogonaliteitscompressie: de fase waarin we proberen de redundantie in de factormatrices te benutten. We weten namelijk dat deze matrices orthogonaal zijn, dus het aantal vrijheidsgraden per matrix ligt lager dan het aantal waarden in de matrix. Eerst probeerden we onze eigen methode met stelsels te ontwikkelen, waarbij een vast deel van de factormatrix werd weggelaten en dan gereconstrueerd aan de hand van een reeks lineaire stelsels. Deze aanpak was echter traag en had grote problemen qua precisie, die deels verholpen konden worden met enkele technieken (waarvan sommige wel een negatief effect hadden op de compressiefactor). De ideale oplossing bleek echter de methode met Householder-reflecties te zijn, gebaseerd op het algoritme voor het berekenen van QR-factorizaties aan de hand van deze reflecties \cite{ref:qr_factorization_householder}. Hiermee slaagden we erin om op een effici\"ente wijze exact het aantal waarden op te slaan dat we voor ogen hadden, zonder dat we een merkbare fout toevoegen aan de factormatrices.\\

De derde fase van ons compressie-algoritme bestaat uit quantisatie: het voorstellen van floating-point waarden met beperkte discrete verzamelingen getallen. Zowel voor het quantiseren van de kerntensor als de factormatrices bleek het erg nuttig om deze op te splitsen in lagen en gebruik te maken van de inherente structuur van deze objecten. Zo bevinden veruit grootste absolute waarden in de kerntensor zich in de posities met lage indices en heeft een vaste absolute fout op elke positie een soortgelijk effect op de globale fout. Bij de factormatrices daarentegen zijn alle waarden net genormaliseerd maar dienen de eerste kolommen met hogere precisie opgeslagen te worden aangezien deze vermenigvuldigd worden met grotere getallen uit de kerntensor. Door een combinatie van dergelijke redeneringen en experimentele resultaten ontwikkelden we specifieke technieken voor het quantiseren van de kerntensor en factormatrices.\\

Ten slotte worden in de encoderingsfase alle gequantiseerde waarden omgezet naar binaire voorstellingen, aan elkaar gehangen en finaal gecomprimeerd met het Deflate-algoritme \cite{ref:deflate}. Deze encodering kan ook nog eens op verschillende manieren gebeuren: met codes van constante lengte (zoals de standaard binaire voorstelling of Gray-codes \cite{ref:graycode}) of Huffman-codes \cite{ref:huffman_coding}. Deze laatste methode bleek erg nuttig voor het comprimeren van de ge\"encodeerde data, maar helaas kostte het een significante hoeveelheid geheugen om bij elke quantisatieblok een expliciete Huffman-boom op te slaan. Als oplossing hiervoor ontwikkelden we ook ons eigen alternatief, benaderende Huffman-codes, en hoewel deze techniek kleine verbeteringen kon boeken op vlak van compressie, was dit wel met een grote kost in rekentijd, waardoor we hier geen verder gebruik van maakten. Uiteindelijk besloten we te werken met een adaptieve encoderingsmethode, waarbij elke blok ge\"encodeerd werd met een Gray-code of Huffman-code met bijbehorende boom, afhankelijk van wat het meest effici\"ent is.\\

Na het maken van deze ontwerpbeslissingen blijven er nog drie parameters over die de uiteindelijke compressiefout en -factor be\"invloeden: de relatieve doelfout in de ST-HOSVD (RDS), de bits-parameter voor de quantisatie van de kerntensor (BPK) en de bits-parameter voor de quantisatie van de factormatrices (BPF). We voerden grootschalige experimenten uit om meer te leren over de evolutie van de optimale parameters voor verschillende uiteindelijke relatieve fouten en datasets. Hoewel de optimale RDS-waarde voor een bepaalde gewenste compressiefout vrij gemakkelijk benaderd kon worden, bleek dit moeilijker voor de BPK en BPF. Hiervoor ontwikkelden we ook een adaptief algoritme, dat iteratief nieuwe compressies test terwijl deze twee laatste parameters verlaagd worden, totdat de gewenste fout bereikt wordt. Helaas vraagt deze adaptieve methode wel te veel rekentijd voor het verwerken van grote datasets.

\newpage
\subsection{Compressie na hervorming}

Men kan, in plaats van te werken met een 3D-tensor, dezelfde hyperspectrale afbeeldingen voorstellen als een 5D-tensor aan de hand van hervorming. We onderzochten of we hiervoor effici\"entere compressiemethoden konden ontwikkelen.\\

Ten eerste bekeken we kort wat het effect is van Tucker-gebaseerde compressie op een dergelijke 5D-tensor. Alleen na het uitvoeren van de eerste compressiefase, de ST-HOSVD, bleek deze methode al significant slechter dan Tucker zonder hervorming, dus we hebben de andere fases niet verder geanalyseerd.\\

Daarnaast onderzochten we ook compressie aan de hand van een nieuwe decompositie, genaamd \textit{tensor trains}, speciaal bedoeld voor het comprimeren van hoog-dimensionale tensoren. Hierbij waren de resultaten na het uitvoeren van de ST-HOSVD veelbelovend. Bijgevolg werkten we ook de andere fases van dit compressie-algoritme uit, analoog aan hoofdstuk \ref{hoofdstuk:tucker}. De meeste gebruikte technieken werden overgenomen van Tucker-gebaseerde compressie, maar enkele aanpassingen, voornamelijk aan de parameterselectie, werden doorgevoerd om rekening te houden met de verschillende eigenschappen van tensor trains ten opzichte van de Tucker-decompositie.

\subsection{Resultaten}

Als we onze Tucker-gebaseerde en tensor-train-gebaseerde compressie-algoritmen vergelijken, bleken tensor trains over het algemeen beter te scoren zowel op vlak van compressietijd en de afweging tussen compressiefactor en -fout. Wanneer we onze resultaten vergeleken met algemene lossy compressietechnieken, zagen we dat onze simpele JPEG-methode het zoals verwacht veel slechter deed, maar dat videocompressie aan de hand van x264 \cite{ref:x264} erg goede resultaten boekte, vergelijkbaar met de onze. Op vlak van compressietijd bleek videocompressie meer of minder tijd dan tensor trains nodig te hebben afhankelijk de comprimeerbaarheid van of de dataset in kwestie. Qua decompressietijd waren tensor trains echter altijd trager.\\

Daarnaast vergeleken we ook tensor-train-compressie en videocompressie met een alternatieve hyperspectrale compressiemethode van Karami et al. \cite{ref:karami}, gebaseerd op de 3D-DCT \cite{ref:dct} en SVM's \cite{ref:svm}. Onze tensor-train-methode bleek voor grotere fouten beter te werken dan deze referentie. Ten slotte toonden we ook een aantal voorbeelden van gecomprimeerde hyperspectrale afbeeldingen, die men kan terugvinden aan het einde van hoofdstuk \ref{hoofdstuk:resultaten}.

\newpage
\section{Verder onderzoek}

Om uiteindelijk aan een redelijke hoeveelheid inhoud voor deze tekst te komen, hebben we ons tijdens deze thesis vaak beperkt in de breedte of diepgang van ons onderzoek. Er zijn dus een aantal zaken die niet behandeld werden maar ons wel interessant lijken:

\begin{itemize}
\item \textbf{Keuze doelfout per mode:} In de ST-HOSVD proberen we de doelfout gelijk te verdelen over de verschillende modes. Het is echter mogelijk dat sommige modes deze fout beter kunnen benutten en dat een ongelijke verdeling van de doelfout leidt tot een grotere compressiefactor.
\item \textbf{Keuze steekproefgrootte bij de randomized SVD:} We zagen in hoofdstuk \ref{hoofdstuk:tucker} enkele mooie resultaten van de randomized SVD, maar gebruikten deze methode uiteindelijk niet omdat het ons moeilijk leek om met beperkt rekenwerk een goede steekproefgrootte te voorspellen. Indien hiervoor technieken ontwikkeld worden, zou deze methode wel degelijk gebruikt kunnen worden om de SVD (in sommige gevallen) verder te versnellen.
\item \textbf{Verbeterde quantisatie:} Het is goed dat we bij onze quantisatie onderscheid maakten tussen het effect van verschillende waarden op het eindresultaat. Men kan hier echter veel verder in gaan. Zo was onze keuze voor een pseudo-lineaire quantisatiefunctie erg simpel en kan men ook rigoreuzer analyseren hoeveel bits men best gebruikt per waarde in functie van een kwaliteitsparameter. Voor meer informatie kan men bijvoorbeeld de Wikipedia-pagina over quantisatie \cite{ref:quantization} bekijken.
\item \textbf{Verbeterde Huffman-encodering:} In het Deflate-algoritme \cite{ref:deflate} wordt, zoals bij onze adaptieve encodering, gebruik gemaakt van verschillende encoderingsstrategie\"en voor verschillende blokken data: geen speciale encodering, Huffman-encodering met een expliciet opgeslagen code en Huffman-encodering met een vaste code. Deze laatste optie zou erg nuttig kunnen zijn in onze algoritmes als de waarden per quantisatieblok over het algemeen dezelfde verdeling volgen voor verschillende blokken en datasets. Dit vereist eerst echter aangepaste quantisatie.
\item \textbf{Verbeterde parameterselectiefuncties:} Bij het automatisch afstellen van parameters hebben we de selectiefuncties beperkt tot een erg simpele vorm. Het is mogelijk dat men zonder adaptieve technieken betere parameters kan kiezen aan de hand van verbeterde selectiefuncties.
\item \textbf{Laag-niveau implementatie van quantisatie en encodering:} Zelfs met het gebruik van \texttt{bitarray} \cite{ref:bitarray} gebeurt er bij de quantisatie en encodering nog steeds erg veel in hoog-niveau Python-code. Het zou interessant zijn om te weten hoeveel men de compressie- en decompressietijd zou kunnen verbeteren door dit te implementeren in een taal als C++.
\end{itemize}